{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Project_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uv9bHjFF-gS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "4eea2ca3-4e17-4299-8e9b-09fc8bb47a66"
      },
      "source": [
        "\"\"\"\n",
        "Project 6 README\n",
        "=====================================\n",
        "\n",
        "Name:\n",
        "Classifier Performance on Real-World Data\n",
        "\n",
        "Purpose:\n",
        "\n",
        "In this project we will run into a new set of challenges when working \n",
        "with a “real-world” dataset, and see how an imbalanced \n",
        "dataset can influence classifier performance.\n",
        "\n",
        "Authors:\n",
        "\n",
        "Brandon Ryan, Yi Wei Lee, Mina Moslehpour\n",
        "\n",
        "Last modified by Brandon Ryan, October 14 2020\n",
        "\n",
        "Experiments:\n",
        "\n",
        "1.  Download bank-additional.zip and extract its contents. Since the dataset is \n",
        "    large and some of the algorithms we will use can be time-consuming, we will \n",
        "    train with bank-additional.csv, which is a subset of the original dataset.\n",
        "\n",
        "    Once our models are trained, we will test against the full dataset, which is in \n",
        "    bank-additional-full.csv.\n",
        "\n",
        "    The archive also contains a text file, bank-additional-names.txt, which \n",
        "    describes the dataset and what each column represents.\n",
        "\n",
        "2.  Use read_csv() to load and examine the training and test sets. Unlike most \n",
        "    CSV files, the separator is actually ';' rather than ','.\n",
        "\n",
        "3.  The training and test DataFrames will need some significant preprocessing \n",
        "    before they can be used:\n",
        "    a. Several of the features are categorical variables and will need to be \n",
        "       turned into numbers before they can be used by ML algorithms. \n",
        "       The simplest way to accomplish this is to use dummy coding using get_dummies().\n",
        "\n",
        "       Some algorithms (e.g. logistic regression) have problems with collinear \n",
        "       features. If you use one-hot encoding, one dummy variable will be a linear \n",
        "       combination of the other dummy variables, so be sure to pass drop_first=True.\n",
        "\n",
        "    b. Per bank-additional-names.txt, the feature duration “should be discarded \n",
        "       if the intention is to have a realistic predictive model,” so removed.\n",
        "\n",
        "    c. The feature y (or y_yes after dummy coding) is the target, so should be removed.\n",
        "\n",
        "    d. Some algorithms (e.g. KNN and SVM) require non-categorical features to be standardized. \n",
        "\n",
        "4.  Fit Naive Bayes, KNN, and SVM classifiers to the training set, then score \n",
        "    each classifier on the test set. Which classifier has the highest accuracy?\n",
        "\n",
        "5.  These numbers look pretty good, but let’s take another look at the data. \n",
        "    How many values in the training set have y_yes = 0, and how many have \n",
        "    y_yes = 1? What would be the accuracy if we simply assumed \n",
        "    that no customer ever subscribed to the product?\n",
        "\n",
        "6.  Use np.zeros_like() to create a target vector representing the output of \n",
        "    the “dumb” classifier of experiment (5), then create a confusion matrix \n",
        "    and find its AUC.\n",
        "\n",
        "7.  Create a confusion matrix and find the AUC for each of the classifiers \n",
        "    of experiment (4). Is the best classifier the one with the highest accuracy?\n",
        "\n",
        "8.  One of the easiest ways to deal with an unbalanced dataset is random oversampling. \n",
        "    This can be done with an imblearn.over_sampling.RandomOverSampler object. \n",
        "    Use fit_resample() to generate a balanced training set.\n",
        "\n",
        "9.  Repeat experiments (4) and (7) on the balanced training set of experiment (8).\n",
        "    Which classifier performs the best, and how much better is its performance?\n",
        "\n",
        "Platforms:\n",
        "\n",
        "Jupyter Notebook\n",
        "\n",
        "Libraries:\n",
        "\n",
        "scikit-learn \n",
        "pandas\n",
        "Matplotlib’s\n",
        "numpy\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nProject 6 README\\n=====================================\\n\\nName:\\nClassifier Performance on Real-World Data\\n\\nPurpose:\\n\\nIn this project we will run into a new set of challenges when working \\nwith a “real-world” dataset, and see how an imbalanced \\ndataset can influence classifier performance.\\n\\nAuthors:\\n\\nBrandon Ryan, Yi Wei Lee, Mina Moslehpour\\n\\nLast modified by Brandon Ryan, October 14 2020\\n\\nExperiments:\\n\\n1.  Download bank-additional.zip and extract its contents. Since the dataset is \\n    large and some of the algorithms we will use can be time-consuming, we will \\n    train with bank-additional.csv, which is a subset of the original dataset.\\n\\n    Once our models are trained, we will test against the full dataset, which is in \\n    bank-additional-full.csv.\\n\\n    The archive also contains a text file, bank-additional-names.txt, which \\n    describes the dataset and what each column represents.\\n\\n2.  Use read_csv() to load and examine the training and test sets. Unlike most \\n    CSV files, the separator is actually ';' rather than ','.\\n\\n3.  The training and test DataFrames will need some significant preprocessing \\n    before they can be used:\\n    a. Several of the features are categorical variables and will need to be \\n       turned into numbers before they can be used by ML algorithms. \\n       The simplest way to accomplish this is to use dummy coding using get_dummies().\\n\\n       Some algorithms (e.g. logistic regression) have problems with collinear \\n       features. If you use one-hot encoding, one dummy variable will be a linear \\n       combination of the other dummy variables, so be sure to pass drop_first=True.\\n\\n    b. Per bank-additional-names.txt, the feature duration “should be discarded \\n       if the intention is to have a realistic predictive model,” so removed.\\n\\n    c. The feature y (or y_yes after dummy coding) is the target, so should be removed.\\n\\n    d. Some algorithms (e.g. KNN and SVM) require non-categorical features to be standardized. \\n\\n4.  Fit Naive Bayes, KNN, and SVM classifiers to the training set, then score \\n    each classifier on the test set. Which classifier has the highest accuracy?\\n\\n5.  These numbers look pretty good, but let’s take another look at the data. \\n    How many values in the training set have y_yes = 0, and how many have \\n    y_yes = 1? What would be the accuracy if we simply assumed \\n    that no customer ever subscribed to the product?\\n\\n6.  Use np.zeros_like() to create a target vector representing the output of \\n    the “dumb” classifier of experiment (5), then create a confusion matrix \\n    and find its AUC.\\n\\n7.  Create a confusion matrix and find the AUC for each of the classifiers \\n    of experiment (4). Is the best classifier the one with the highest accuracy?\\n\\n8.  One of the easiest ways to deal with an unbalanced dataset is random oversampling. \\n    This can be done with an imblearn.over_sampling.RandomOverSampler object. \\n    Use fit_resample() to generate a balanced training set.\\n\\n9.  Repeat experiments (4) and (7) on the balanced training set of experiment (8).\\n    Which classifier performs the best, and how much better is its performance?\\n\\nPlatforms:\\n\\nJupyter Notebook\\n\\nLibraries:\\n\\nscikit-learn \\npandas\\nMatplotlib’s\\nnumpy\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysbvg4jDONsn"
      },
      "source": [
        "### 1. Download bank-additional.zip and extract its contents. Since the dataset is large and some of the algorithms we will use can be time-consuming, we will train with bank-additional.csv, which is a subset of the original dataset.\n",
        "###Once our models are trained, we will test against the full dataset, which is in bank-additional-full.csv.\n",
        "###The archive also contains a text file, bank-additional-names.txt, which describes the dataset and what each column represents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIMpbgEIF-gh",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "3f8163e7-a634-4ff2-c3c4-4831f4c402a4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plotter\n",
        "from matplotlib import colors\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing  import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import io\n",
        "#upload data to the google colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "#uploaded = files.upload()\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aecf98b5-9040-4ef3-b688-5e9b580bd06b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aecf98b5-9040-4ef3-b688-5e9b580bd06b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving bank-additional.csv to bank-additional (2).csv\n",
            "'bank-additional (1).csv'  'bank-additional-full (1).csv'\n",
            "'bank-additional (2).csv'   bank-additional-full.csv\n",
            " bank-additional.csv\t    sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzkysqrxk5kN"
      },
      "source": [
        "### 2. Use read_csv() to load and examine the training and test sets. Unlike most CSV files, the separator is actually ';' rather than ','.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOu4TnyEWOVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2750995a-aa14-4138-fc59-72eb0086afd7"
      },
      "source": [
        "pd.set_option('display.max_columns', 30)\n",
        "\n",
        "# Data pulled from: 'https://archive.ics.uci.edu/ml/machine-learning-databases/00222/'\n",
        "bank_data = pd.read_csv(\"bank-additional.csv\", sep = ';')\n",
        "#print(\"Bank Data: \")\n",
        "print(bank_data.shape)\n",
        "\n",
        "#bank_data_full = pd.read_csv(\"bank-additional-full.csv\", sep = ';')\n",
        "#print(\"Bank Data Full: \")\n",
        "#print(bank_data_full)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4119, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bClth62kqks"
      },
      "source": [
        "###3. The training and test DataFrames will need some significant preprocessing before they can be used:\n",
        "\n",
        "###Several of the features are categorical variables and will need to be turned into numbers before they can be used by ML algorithms. The simplest way to accomplish this is to use dummy coding using get_dummies().\n",
        "\n",
        "###Some algorithms (e.g. logistic regression) have problems with collinear features. If you use one-hot encoding, one dummy variable will be a linear combination of the other dummy variables, so be sure to pass drop_first=True.\n",
        "\n",
        "###Per bank-additional-names.txt, the feature duration “should be discarded if the intention is to have a realistic predictive model,” so removed.\n",
        "\n",
        "###The feature y (or y_yes after dummy coding) is the target, so should be removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "a73JaWOUF-g6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcdfdd91-0ad9-4b73-ed76-16529542beea"
      },
      "source": [
        "job = pd.get_dummies(bank_data['job'], drop_first=True)\n",
        "marital = pd.get_dummies(bank_data['marital'], drop_first=True)\n",
        "education = pd.get_dummies(bank_data['education'], drop_first=True)\n",
        "default = pd.get_dummies(bank_data['default'], drop_first=True)\n",
        "housing = pd.get_dummies(bank_data['housing'], drop_first=True)\n",
        "loan = pd.get_dummies(bank_data['loan'], drop_first=True)\n",
        "contact = pd.get_dummies(bank_data['contact'], drop_first=True)\n",
        "month = pd.get_dummies(bank_data['month'], drop_first=True)\n",
        "day_of_week = pd.get_dummies(bank_data['day_of_week'], drop_first=True)\n",
        "poutcome = pd.get_dummies(bank_data['poutcome'], drop_first=True)\n",
        "\n",
        "# Removing 'duration' column from bank_data\n",
        "bank_data = bank_data.drop(columns='duration')\n",
        "\n",
        "# Removing y (target) from bank_data\n",
        "y = pd.get_dummies(bank_data['y'])\n",
        "bank_data = bank_data.drop(columns='y')\n",
        "\n",
        "# Preprocessing functions using label encoder and one-hot encoder\n",
        "bank_data = bank_data.select_dtypes(include=[object])\n",
        "le = preprocessing.LabelEncoder()\n",
        "bank_data_le = bank_data.apply(le.fit_transform)\n",
        "print(bank_data_le)\n",
        "\n",
        "enc = preprocessing.OneHotEncoder()\n",
        "enc.fit(bank_data_le)\n",
        "onehotlabels = enc.transform(bank_data_le).toarray()\n",
        "print(onehotlabels.shape)\n",
        "\n",
        "#bank_data = bank_data.join(job)\n",
        "#bank_data = bank_data.join(marital)\n",
        "#bank_data = bank_data.join(education)\n",
        "#bank_data = bank_data.join(default)\n",
        "#ank_data = bank_data.join(housing)\n",
        "#bank_data = bank_data.join(loan)\n",
        "#bank_data = bank_data.join(contact)\n",
        "#bank_data = bank_data.join(month)\n",
        "#bank_data = bank_data.join(day_of_week)\n",
        "#bank_data = bank_data.join(poutcome)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      job  marital  education  default  housing  loan  contact  month  \\\n",
            "0       1        1          2        0        2     0        0      6   \n",
            "1       7        2          3        0        0     0        1      6   \n",
            "2       7        1          3        0        2     0        1      4   \n",
            "3       7        1          2        0        1     1        1      4   \n",
            "4       0        1          6        0        2     0        0      7   \n",
            "...   ...      ...        ...      ...      ...   ...      ...    ...   \n",
            "4114    0        1          1        0        2     2        0      3   \n",
            "4115    0        1          3        0        2     0        1      3   \n",
            "4116    8        2          3        0        0     0        0      6   \n",
            "4117    0        1          3        0        0     0        0      1   \n",
            "4118    4        2          3        0        2     0        0      7   \n",
            "\n",
            "      day_of_week  poutcome  \n",
            "0               0         1  \n",
            "1               0         1  \n",
            "2               4         1  \n",
            "3               0         1  \n",
            "4               1         1  \n",
            "...           ...       ...  \n",
            "4114            2         1  \n",
            "4115            0         1  \n",
            "4116            1         0  \n",
            "4117            0         1  \n",
            "4118            4         1  \n",
            "\n",
            "[4119 rows x 10 columns]\n",
            "(4119, 53)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQzgKJVulKNp"
      },
      "source": [
        "### 4. Fit Naive Bayes, KNN, and SVM classifiers to the training set, then score each classifier on the test set. Which classifier has the highest accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHeMyW3XeNy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e223d7-2bb0-4b65-d1ea-baa8bc7af50b"
      },
      "source": [
        "# Gaussian Naive Bayes classifiers for bank_data using y_yes as target data\n",
        "Y = y['yes']\n",
        "X = onehotlabels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.8, random_state=0)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_nor = sc.transform(X_train)\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_train_nor,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8857837181044957"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjBK2XJDiJ13",
        "outputId": "59883fe6-6544-4ad8-9043-cbaa5577126d"
      },
      "source": [
        "# K-nearest neighbor classifiers for bank_data using y_yes as target data\n",
        "Y = y['yes']\n",
        "X = onehotlabels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.8, random_state=0)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_nor = sc.transform(X_train)\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=3)\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_train_nor,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.905224787363305"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdZt0pqfiawS",
        "outputId": "ddf95937-c4c4-47cc-b9cd-ace223e1ecfb"
      },
      "source": [
        "# SVM classifiers for bank_data using y_yes as target data\n",
        "Y = y['yes']\n",
        "X = onehotlabels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.8, random_state=0)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_nor = sc.transform(X_train)\n",
        "model = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9100850546780073"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5zy7ysur2vl"
      },
      "source": [
        "The SVM Classifier has the highest accuracy of all 3 classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUqKiGEulRZK"
      },
      "source": [
        "### 5. These numbers look pretty good, but let’s take another look at the data. How many values in the training set have y_yes = 0, and how many have y_yes = 1? What would be the accuracy if we simply assumed that no customer ever subscribed to the product?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKnIhxIK-LhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fe9202-268b-46cf-9d69-ff84e1efa806"
      },
      "source": [
        "y_yes = y['yes'].tolist()\n",
        "print(\"y_yes = 0 occurs: {}\".format(y_yes.count(0)))\n",
        "print(\"y_yes = 1 occurs: {}\".format(y_yes.count(1)))\n",
        "\n",
        "# Gaussian Naive Bayes classifiers for bank_data using y_yes as target data with 0 for all values\n",
        "\n",
        "Y = np.zeros_like(y['yes'])\n",
        "Y[0] = 1\n",
        "X = onehotlabels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.8, random_state=0)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_nor = sc.transform(X_train)\n",
        "\n",
        "print('\\nIf we assume no customer ever subscribed to the product then:\\n')\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "print('GNB = {}'.format(model.score(X_train_nor,y_train)))\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=3)\n",
        "model.fit(X_train, y_train)\n",
        "print('KNN = {}'.format(model.score(X_train_nor,y_train)))\n",
        "\n",
        "model = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "model.fit(X_train, y_train)\n",
        "print('SVM = {}'.format(model.score(X_train,y_train)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_yes = 0 occurs: 3668\n",
            "y_yes = 1 occurs: 451\n",
            "\n",
            "If we assume no customer ever subscribed to the product then:\n",
            "\n",
            "GNB = 0.9987849331713244\n",
            "KNN = 0.9987849331713244\n",
            "SVM = 0.9987849331713244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zEPFjv8tYv1"
      },
      "source": [
        "If we simply assumed that no customer ever subscribed to the product then the accuracy would be 1.0 for all classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmL0nkZUlXpB"
      },
      "source": [
        "### 6. Use np.zeros_like() to create a target vector representing the output of the “dumb” classifier of experiment (5), then create a confusion matrix and find its AUC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9agKhDBm-Pxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa8b9a7c-2ef4-49be-dfb6-c692f7805050"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "Y_true = y['yes']\n",
        "Y_pred = np.zeros_like(y['yes'])\n",
        "\n",
        "print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(Y_true, Y_pred)))\n",
        "print('AUC of confusion matrix: {}'.format(roc_auc_score(Y_true, Y_pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[3668    0]\n",
            " [ 451    0]]\n",
            "\n",
            "AUC of confusion matrix: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvTTg0g9ld7S"
      },
      "source": [
        "### 7. Create a confusion matrix and find the AUC for each of the classifiers of experiment (4). Is the best classifier the one with the highest accuracy?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dEpThrHkVPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e5a806-a299-46e7-a339-6233d7b9e041"
      },
      "source": [
        "# Gaussian Naive Bayes classifiers for bank_data using y_yes as target data\n",
        "Y = y['yes']\n",
        "X = onehotlabels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.8, random_state=0)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_nor = sc.transform(X_train)\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "print('GNB Confusion Matrix:\\n{}\\n'.format(confusion_matrix(Y_true, Y_pred)))\n",
        "print('AUC of confusion matrix: {}'.format(roc_auc_score(Y_true, Y_pred)))\n",
        "\n",
        "# K-nearest neighbor classifiers for bank_data using y_yes as target data\n",
        "model = KNeighborsClassifier(n_neighbors=3)\n",
        "model.fit(X_train, y_train)\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "print('GNB Confusion Matrix:\\n{}\\n'.format(confusion_matrix(Y_true, Y_pred)))\n",
        "print('AUC of confusion matrix: {}'.format(roc_auc_score(Y_true, Y_pred)))\n",
        "\n",
        "# SVC classifiers for bank_data using y_yes as target data\n",
        "model = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "model.fit(X_train, y_train)\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "print('GNB Confusion Matrix:\\n{}\\n'.format(confusion_matrix(Y_true, Y_pred)))\n",
        "print('AUC of confusion matrix: {}'.format(roc_auc_score(Y_true, Y_pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GNB Confusion Matrix:\n",
            "[[ 725 2943]\n",
            " [  36  415]]\n",
            "\n",
            "AUC of confusion matrix: 0.5589163908145476\n",
            "GNB Confusion Matrix:\n",
            "[[3570   98]\n",
            " [ 392   59]]\n",
            "\n",
            "AUC of confusion matrix: 0.5520514209305868\n",
            "GNB Confusion Matrix:\n",
            "[[3664    4]\n",
            " [ 422   29]]\n",
            "\n",
            "AUC of confusion matrix: 0.5316055197827679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFtUOZ_0x1_2"
      },
      "source": [
        "No, in fact the results from the AUC of each confusion matrix seems to be inversely proportional to the classifiers that had higher accuracies. AUC of GNB is the best and scored the worst in terms of accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9vU95wRlmSb"
      },
      "source": [
        "### 8. One of the easiest ways to deal with an unbalanced dataset is random oversampling. This can be done with an imblearn.over_sampling.RandomOverSampler object. Use fit_resample() to generate a balanced training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQxtP2AjkWEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7851fa70-0a69-498a-a477-591a412a848e"
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Using the fit_resample on the training data\n",
        "Y = y['yes']\n",
        "X = onehotlabels\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_res, Y_res = ros.fit_resample(X, Y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, Y_res, test_size=0.8, random_state=0)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_nor = sc.transform(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDPDiydfl1sQ"
      },
      "source": [
        "### 9. Repeat experiments (4) and (7) on the balanced training set of experiment (8). Which classifier performs the best, and how much better is its performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGiGR91BkWkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe9bdbc-ddfb-43b2-8efa-004b138dcfb9"
      },
      "source": [
        "# Gaussian Naive Bayes classifier\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "print('GNB Score = {}'.format(model.score(X_train,y_train)))\n",
        "print('GNB Confusion Matrix:\\n{}'.format(confusion_matrix(Y_true, Y_pred)))\n",
        "print('AUC of confusion matrix: {}\\n'.format(roc_auc_score(Y_true, Y_pred)))\n",
        "\n",
        "# K-nearest neighbor classifier\n",
        "model = KNeighborsClassifier(n_neighbors=3)\n",
        "model.fit(X_train, y_train)\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "print('KNN Score = {}'.format(model.score(X_train,y_train)))\n",
        "print('KNN Confusion Matrix:\\n{}'.format(confusion_matrix(Y_true, Y_pred)))\n",
        "print('AUC of confusion matrix: {}\\n'.format(roc_auc_score(Y_true, Y_pred)))\n",
        "\n",
        "# SVC classifier\n",
        "model = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "model.fit(X_train, y_train)\n",
        "Y_pred = model.predict(X)\n",
        "\n",
        "print('SVC Score = {}'.format(model.score(X_train,y_train)))\n",
        "print('SVC Confusion Matrix:\\n{}'.format(confusion_matrix(Y_true, Y_pred)))\n",
        "print('AUC of confusion matrix: {}\\n'.format(roc_auc_score(Y_true, Y_pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GNB Score = 0.6203135650988412\n",
            "GNB Confusion Matrix:\n",
            "[[1507 2161]\n",
            " [  84  367]]\n",
            "AUC of confusion matrix: 0.6122989140816362\n",
            "\n",
            "KNN Score = 0.8841172460804363\n",
            "KNN Confusion Matrix:\n",
            "[[2580 1088]\n",
            " [  94  357]]\n",
            "AUC of confusion matrix: 0.7474774341279647\n",
            "\n",
            "SVC Score = 0.841854124062713\n",
            "SVC Confusion Matrix:\n",
            "[[3160  508]\n",
            " [ 152  299]]\n",
            "AUC of confusion matrix: 0.7622380412363656\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pn480IH0zWG"
      },
      "source": [
        "The SVC classifier performs best with the newly balanced data set with a slight margin over the KNN classifier."
      ]
    }
  ]
}
